{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Color Quantization\n",
    "\n",
    "<font size=\"6\"> Laboratory 3 </font> <br>\n",
    "<font size=\"3\"> Last updated June 16, 2022 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 00. Content </span>\n",
    "\n",
    "<font size=\"5\"> Mathematics </font>\n",
    "- 3 dimensional vectors\n",
    "- Norm of vectors\n",
    "- 1D quantization\n",
    "- Euclidean distance\n",
    "- 3D transforms\n",
    "- Differential equations\n",
    "\n",
    "<font size=\"5\"> Programming Skills </font>\n",
    "- Multi-dimensional array manipulation\n",
    "- Functions\n",
    "\n",
    "<font size=\"5\"> Embedded Systems </font>\n",
    "- Thonny and MicroPython\n",
    "\n",
    "## <span style=\"color:orange;\"> 0. Required Hardware </span>\n",
    "- Microcontroller: Raspberry Pi Pico\n",
    "- Breadboard\n",
    "- USB connector\n",
    "- NeoPixels\n",
    "- Level shifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:lightblue\"> Write your name and email below: </h3>\n",
    "\n",
    "**Name:** me \n",
    "\n",
    "**Email:** me @purdue.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 1. Grayscale/Desaturation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Grayscaling\" an image, also called desaturation, is when all the color in the image is transformed to appear as a shade of gray without apparent color. Specifically, these shades represent the intensity information of the light. This can be done for a variety of reasons, from taking up less storage space to aesthetics.\n",
    "\n",
    "When coding this transformation, there are a multitude of different methods that can be used, many of which involve calculus principles. However, some of the mathematics that may be assumed to work don't produce accurate results. Even if the resulting image looks like its been grayscaled, some of the darks or lights might be too pronounced, or the shadows might be outlined, causing the blending to look awkward.\n",
    "\n",
    "Lets try some different methods to produce new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show original and grayscaled images\n",
    "def showimage(img,new):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    ax[0].imshow(np.array(img))\n",
    "    ax[0].axis('off')   \n",
    "    ax[1].imshow(np.array(new), cmap='gray')\n",
    "    ax[1].axis('off')            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude\n",
    "\n",
    "One of the methods that may be assumed to work is taking the length, or magnitude of the image array. The magnitude of a vector can be used to find the unit vector, which has a length of 1. Similarly, grayscaling works by transforming the pixels so they only represent the intensity/brightness information of the light. Intensity is shown in an image through blacks, whites, and grays, which results in the image looking to be shades of these three \"colors.\" By taking the magnitude and applying it to the original image array values, it can be reasoned that the resultant values would only represent a portion of the image's characteristics.\n",
    "\n",
    "In this case we will be taking the dot product of the image array and the magnitude to produce a gray image, however, afterwards feel free to check what happens when you take the image array and divide it by the magnitude in a similar manner to finding the unit tangent vector. It produces a really interesting image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 1 </span>\n",
    "\n",
    "Take the magnitude of RGB for the pixels of an image and find the dot product of the image array and the magnitude.\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 1 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitudemethod(img):\n",
    "    img = np.array(img)\n",
    "    for i in range(0, len(img)):\n",
    "        mag =  # fill in here\n",
    "    img = np.dot(img, mag)\n",
    "    return img\n",
    "\n",
    "img = Image.open('mnms.jpg')\n",
    "new = magnitudemethod(img)\n",
    "showimage(img,new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this might seem like a perfectly reasonable solution, since everything is grayed over and the highlights and shadows match the original picture. However, some of colors are actually completely off, such as the blue colors being too light while the yellows are too dark. Along with this, the shadows and highlights don't blend very well, which results in some of the objects looking awkward. These issues might seem minimal, with some of them not even being noticeable until compared with a more proper solution (which will be done later), but they are nonetheless issues, meaning this is not the best solution for grayscaling an image.\n",
    "\n",
    "### RGB Versus HSV\n",
    "\n",
    "These issues may be more apparent when using the HSV (hue, saturation, value) values for the image. You may have seen HSV value options when using a printer or image editing software. While RGB defines color in terms of the combination of the primary, colors red, green, and blue, HSV deals with lighting and tints. HSV is more sensitive to changes in lighting and is more helpful when making detailed and realistic modifications to an image. In this course, we'll be primarily using RGB since it is more common for smaller changes like we're doing and is also more well-known as a concept.\n",
    "\n",
    "However, HSV is actually the closest scheme to how humans perceive color, in comparison to RGB and CMYK (cyan, magenta, yellow, key; another color scheme). Hue is the portion of the model and is expressed as a number from 0 to 360 degrees, with primary colors falling in certain degree ranges. Saturation describes the amount of gray in a color and ranges from 0 to 100%, where the closer it gets to 0%, the more gray a color is. Value, also called brightness in some softwares (HSB), describes the intensity of a color and is also defined from 0 to 100%, with 0% being black/the darkest and 100% being white/the brightest, meaning it reveals the most color. \n",
    "\n",
    "Below is grayscaling using the magnitude method again, but this time with the image defined by HSV values. As can be seen, the light blues once again are considerably dark for being the lightest color in the image, aside from the background. Looking at the grayscaled image, most people would assume that the light gray objects would be the lightest colors, but they're not, just as was shown with the yellows and the blues in the RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV; reads images in HSV instead of RGB\n",
    "\n",
    "img = cv2.imread('mnms.jpg')\n",
    "new = magnitudemethod(img)\n",
    "showimage(img,new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging\n",
    "\n",
    "Other than magnitude, you may also think that taking the average of the RGB values and applying it to the image would produce a grayscaled image. The goal with grayscaling is to diminish the color values to just their intensities, so, similar to magnitude, it may make sense to try and simply find what the \"middle\" of the colors is. It can also be considered a form of mixing, which is a way that gray-based colors can be found if you think about it in terms of mixing paint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 2 </span>\n",
    "\n",
    "Take the average of RGB for the pixels of an image and find the dot product of the image array and the average.\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 2 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagemethod(img):\n",
    "    img = np.array(img)\n",
    "    for i in range(0, len(img)):\n",
    "        avg =  # fill in here\n",
    "    img = np.dot(img, avg)\n",
    "    return img\n",
    "\n",
    "img = Image.open('mnms.jpg')\n",
    "new = averagemethod(img)\n",
    "showimage(img,new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this method should look very similar to the magnitude method, with blue being the brightest and yellow appearing almost as dark as the red and orange. In this case, the grayscaled image is actually a bit brighter than the magnitude method, though it's not very obvious at first glance.\n",
    "\n",
    "In this case, we will not be taking the average of the HSV values since these values represent different characteristics, unlike how RGB is representative of the combination of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Specific Vector\n",
    "\n",
    "As mentioned before, taking the magnitude or average of the colors doesn't produce a well-grayscaled image for multiple reasons. So what do people normally do when trying to grayscale an image? Again, there are many methods that can be used, but one of the most common, and simplest, ways is to take the dot product of the image values and a specific vector that has the express purpose of manipulating the red, green, and blue values in an image to be gray. The specific vector you see most often is `[0.2989, 0.5870, 0.1140]`, however there are a few other vectors that produce properly grayscaled images, such as `[0.2126, 0.7512, 0.0722]`. Rounding these vectors to the tenth place is also used relatively often in coding, producing `[0.3, 0.6, 0.1]`. \n",
    "\n",
    "Try finding the grayscaled image using the dot product of the image values array and a specific vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 3 </span>\n",
    "\n",
    "Take the dot product of the image array and `[0.2989, 0.5870, 0.1140]`.\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 3 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(img):\n",
    "    img = np.array(img)\n",
    "    return  # fill in here\n",
    "\n",
    "img = Image.open('mnms.jpg')\n",
    "new = rgb2gray(img)\n",
    "showimage(img,new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the resulting grayscaled image to the other methods used, you can better see that the other methods aren't producing the correct images. The correctly grayscaled image has more subdued tones and the colors blend together better. Besides this, the image also has more definition. The other methods produced images where the objects looked nearly flat or 2-dimensional even though they're 3-dimensional objects, which most likely occurs due to a lack of proper blending.\n",
    "\n",
    "Try playing around with the vector values to see what happens to the resulting image. For example, try making one value much larger or smaller than the other two, making all the values extremely large or small, etc. You can get some super weird looking images from doing this!\n",
    "\n",
    "Also take a look at what the grayscaled image of the HSV looks like for comparison and to see what the color intensities convert to a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "img = cv2.imread('mnms.jpg')\n",
    "new = rgb2gray(img)\n",
    "showimage(img,new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the colors are a little bit different, unlike what happened with the magnitude method. However, in the HSV grayscaled image, the different colors are actually a bit easier to pick out, especially in the back of the picture where the lighting caused by the flash is less prevalent. As mentioned before, HSV is more sensitive to lighting differences, which is most likely why the colors are easier to pick apart in the darker part of the picture as compared to the lighter part of the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 2. Quantization </span>\n",
    "\n",
    "We mentioned before that we have $256 \\times 256 \\times 256 = 16,777,216$ colors to work with since there are 256 integers to choose from for a red, green, and blue channel value. This is one form of color quantization since there are infinitely many colors and we've reduced that down to about 16.7 million. In many applications, further quantization is necessary. Quantization is used for image compression by reducing the information of an image so that it requires less storage, and some devices like phones or printers only support a certain palette of colors. \n",
    "\n",
    "Grayscaling, like the transformations you were performing in the first section of this lab, can be considered a form of quantization as you were taking a large collection of different values and converting them to be a smaller set of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we are only able to use 10 gray values instead of the usual 256. We can do this with a quantization algorithm.\n",
    "Uniform quantization is a simple type of quantization where the quantization levels are equally spaced. Here's how the uniform quantization function looks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels=10\n",
    "x = np.linspace(0,255,num_levels+1)\n",
    "y = np.linspace(0,255,num_levels)\n",
    "  \n",
    "plt.step(x, np.append(y,y[-1]), where='post')\n",
    "plt.xlabel('Grayscale Value')\n",
    "plt.ylabel('Quantized Value')\n",
    "plt.title('Uniform Quantization in 1D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 6 </span>\n",
    "\n",
    "In your own words, explain how to read the graph above as if you were talking to a friend who has never learned about color science. \n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 6 Below </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 7 </span>\n",
    "\n",
    "Write a function that converts an image to grayscale and then uniformly quantizes the gray image with $k$ levels. Show your results when $k=2,10,24,$ and $100$.\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 7 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quantize color images. Let's define our own palette of colors and map each to pixel to 1 of 4 options based on Euclidean distance.\n",
    "The distance between the two colors $(R_1,G_1,B_1)$ and $(R_2,G_2,B_2)$ is $\n",
    "    d = \\sqrt{ (R_1-R_2)^2 + (G_1-G_2)^2 + (B_1-B_2)^2 }.$ \n",
    "    \n",
    "### <span style=\"color:red\"> Exercise 8 </span>\n",
    "\n",
    "**Part 1** \n",
    "\n",
    "Choose your own palette of at least 4 colors and write a function that quantizes a color image to your palette based on the shortest Euclidean distance among the chosen colors.\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 8 Part 1 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2** \n",
    "\n",
    "Quantize your image again but this time based on the shortest Euclidean distance in a different color space, namely the CIE L\\*a\\*b\\* space. That is, transform the RGB vectors to L\\*a\\*b\\* vectors and then compute the distance between the two L\\*a\\*b\\* vectors. See the example below on how to convert from RGB to CIE L\\*a\\*b\\*.\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 8 Part 2 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3** \n",
    "\n",
    "Do you notice any differences in the two quantization methods? What if you add more or less colors to your palette?\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 8 Part 3 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are methods for adaptive color quantization. Based on the image, the palette of colors changes. We won't explore them for now, but here's an example where the 'best' 15 colors are chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('italy.jpg')\n",
    "quant_img = img.quantize(colors=15)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax[0].imshow(img)\n",
    "ax[0].axis('off')            \n",
    "ax[1].imshow(quant_img)\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange;\"> 3. Gamma Correction </span>\n",
    "\n",
    "Consider the game of \"guess how many jellybeans (or other small objects) are in the jar\". This game would be very uninteresting if there were only 4 jellybeans in the jar. We would have a much harder time visually distinguishing between 140 and 141 jellybeans, but there is some threshold where, say, 180 jellybeans looks noticeably different from 140 jellybeans. If you've been in a psychology class, you may have heard of [Weber's Law](https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law), which quantifies human perception in response to a stimulus like an increase in jellybeans. Weber's Law suggests that this threshold is proportional to the relative stimulus increment. In other words, if $dV$ is a just perceivable difference in the amount of jellybeans $J$, then for a constant $\\alpha$, $\n",
    "    dV = \\alpha \\frac{dJ}{J}.$\n",
    "Integrating both sides, we get $\n",
    "    V = \\alpha \\ln(J).$\n",
    "Weber's Law also applies to how we perceive light and color. The relationship we just derived suggests that we should quantize color levels logarithmically instead of uniformly so the levels are farther apart as the value increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Exercise 9 </span>\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "Write a script that displays Purdue Old Gold on the NeoPixels. Reference the first lab on color for how to wire up the NeoPixels.\n",
    "\n",
    "It doesn't look quite right. That's because according to Weber's Law we need to adjust the brightness levels for the NeoPixel to be logarithmic. This process is called *gamma correction*. The new gamma corrected RGB values are $\n",
    "    R_{new} = 255(R/255)^\\gamma, \\quad G_{new} = 255(G/255)^\\gamma, \\quad B_{new} = 255(B/255)^\\gamma.$\n",
    "Typically, $\\gamma=2.2$ for cameras and scanners. \n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 9 Part 1 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function that gamma corrects an input of RGB values. What does Purdue gold look like now? Try different values for $1 \\leq \\gamma \\leq 10$. Report your findings. \n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 9 Part 2 Below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Reflection </span>\n",
    "\n",
    "__1. What parts of the lab, if any, do you feel you did well? <br>\n",
    "2. What are some things you learned today? <br>\n",
    "3. Are there any topics that could use more clarification? <br>\n",
    "4. Do you have any suggestions on parts of the lab to improve?__\n",
    "\n",
    "<h3 style=\"background-color:lightblue\"> Write Answers for the Reflection Below </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e50b8cbafc0c2ddeea0b261b330ded4c07fcb971b53272b79f259415b027d80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
